LigthGBM是boosting集合模型中的新进成员，由**微软**提供，它和XGBoost一样是对GBDT的高效实现，原理上它和GBDT及XGBoost类似，都采用**损失函数的负梯度作为当前决策树的残差近似值**，去拟合新的决策树。

LightGBM在很多方面会比XGBoost表现的更为优秀。它有以下优势：

- **更快**的训练效率
- **低内存**使用
- **更高的准确率**
- 支持**并行化学习**
- 可处理**大规模数据**
- 支持**直接使用category特征**

从下图实验数据可以看出， LightGBM比XGBoost快将近10倍，内存占用率大约为XGBoost的1/6，并且准确率也有提升。

![img](https://www.biaodianfu.com/wp-content/uploads/2020/09/comparison.png)

看完这些惊人的实验结果以后，对下面两个问题产生了疑惑：XGBoost已经十分完美了，为什么还要追求速度更快、内存使用更小的模型？对GBDT算法进行改进和提升的技术细节是什么？

## 提出LightGBM的动机

常用的机器学习算法，例如神经网络等算法，都可以以mini-batch的方式训练，训练数据的大小不会受到内存限制。而GBDT在每一次迭代的时候，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间。尤其面对工业级海量的数据，普通的GBDT算法是不能满足其需求的。

![img](https://www.biaodianfu.com/wp-content/uploads/2020/09/gbdt-1.png)

LightGBM提出的主要原因就是为了**解决GBDT在海量数据遇到的问题**，让GBDT可以更好更快地用于**工业实践**。

### XGBoost的优缺点

#### 精确贪心

每轮迭代时，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间。

- 优点：
  - **可以找到精确的划分条件**

- 缺点：
  - **计算量巨大**
  - **内存占用巨大**
  - **易产生过拟合**

#### 预排序

首先，**空间消耗大**。该算法需要**保存数据的特征值及特征排序的结果**（例如排序后的索引，为了后续快速的计算分割点），这里需要消耗训练数据两倍的内存。其次时间上也有较大的开销，在遍历每一个分割点的时候，都需要进行分裂增益的计算，消耗的代价大。

- 优点：
  - **可以使用多线程**
  - **可以加速精确贪心算法**

- 缺点：
  - 效率低下，**可能产生不必要的叶结点**

#### 对cache优化不友好

在预排序后，特征对梯度的访问是一种随机访问，并且不同的特征访问的顺序不一样，无法对cache进行优化。同时，在每一层长树的时候，需要随机访问一个行索引到叶子索引的数组，并且不同特征访问的顺序也不一样，也会造成较大的cache miss

## LightGBM在哪些地方进行了优化？

以上与其说是XGBoost的不足，倒不如说是LightGBM作者们构建新算法时着重瞄准的点。解决了什么问题，那么原来模型没解决就成了原模型的缺点。

概括来说，lightGBM主要有以下特点：

- **基于Histogram的决策树算法**
- **带深度限制的Leaf-wise的叶子生长策略**
- **直方图做差加速**
- **直接支持类别特征(Categorical Feature)**
- **Cache命中率优化**
- **基于直方图的稀疏特征优化**
- **多线程优化**

![img](https://www.biaodianfu.com/wp-content/uploads/2020/09/lightgbm.png)

### 决策树算法

XGBoost使用的是pre-sorted算法，能够更精确的找到数据分隔点。

- 首先，对所有特征按数值进行预排序。
- 其次，在每次的样本分割时，用O(#data)的代价找到每个特征的最优分割点。
- 最后，找到最后的特征以及分割点，将数据分裂成左右两个子节点。

这种pre-sorting算法能够准确找到分裂点，但是在空间和时间上有很大的开销。

- 由于需要对特征进行预排序并且需要保存排序后的索引值（为了后续快速的计算分裂点），因此**内存需要训练数据的两倍**。
- 在遍历每一个分割点的时候，都需要进行分裂增益的计算，消耗的代价大。

LightGBM使用的是**histogram算法**，占用的**内存更低**，数据分割的**复杂度更低**。其思想是**将连续的浮点特征离散成k个离散值**，并构造宽度为k的Histogram。然后遍历训练数据，统计每个离散值在直方图中的累计统计量。在进行特征选择时，只需要根据直方图的离散值，遍历寻找最优的分割点。

![img](https://www.biaodianfu.com/wp-content/uploads/2020/09/histogram.png)

使用直方图算法有很多优点。首先最明显就是**内存消耗的降低**，直方图算法**不仅不需要额外存储预排序的结果，而且可以只保存特征离散化后的值**，而这个值一般用**8位整型**存储就足够了，**内存消耗可以降低为原来的1/8**。

![img](https://www.biaodianfu.com/wp-content/uploads/2020/09/8-bit.png)

然后在**计算上的代价也大幅降低**，预排序算法每遍历一个特征值就需要计算一次分裂的增益，而直方图算法只需要计算k次（k可以认为是常数），**时间复杂度从O(#data #feature) 优化到 O(k #features)**。

**Histogram algorithm**

Histogram algorithm应该翻译为直方图算法，直方图算法的思想也很简单，首先**将连续的浮点数据转换为bin数据**，具体过程是首先确定对于每一个特征需要多少的桶bin，然后均分，将属于该桶的样本数据更新为bin的值，最后用直方图表示。（看起来很高大上，其实就是直方图统计，最后我们将大规模的数据放在了直方图中）

直方图算法相当于**正则化**：

- 使用bin意味着很多数据的细节特征被放弃了，相似的数据可能被划分到相同的桶中，这样的**数据之间的差异就消失了**；
- bin数量选择决定了正则化的程度，**bin越少惩罚越严重，欠拟合风险越高**。

直方图算法需要注意的地方：

- 构建直方图时**不需要对数据进行排序**（比XGBoost快），因为预先设定了bin的范围；
- 直方图除了保存**划分阈值**和**当前bin内样本数**以外还保存了**当前bin内所有样本的一阶梯度和**（一阶梯度和的平方的均值等价于均方损失）；
- 阈值的选取是按照直方图从小到大遍历，使用了上面的一阶梯度和，目的是得到划分之后△loss最大的特征及阈值。

Histogram算法的优缺点：

- Histogram算法并不是完美的。由于特征被离散化后，找到的并不是很精确的分割点，所以会对结果产生影响。但在实际的数据集上表明，离散化的分裂点对最终的精度影响并不大，甚至会好一些。原因在于**decision tree本身就是一个弱学习器，采用Histogram算法会起到正则化的效果，有效地防止模型的过拟合**。
- 时间上的开销由原来的O(#data * #features)降到O(k * #features)。由于离散化，#bin远小于#data，因此时间上有很大的提升。

Histogram算法还可以进一步加速。**一个叶子节点的Histogram可以直接由父节点的Histogram和兄弟节点的Histogram做差得到**。一般情况下，构造Histogram需要遍历该叶子上的所有数据，通过该方法，只需要遍历Histogram的k个捅。速度提升了一倍。

### 决策树生长策略

在Histogram算法之上，LightGBM进行进一步的优化。首先它抛弃了大多数GBDT工具使用的按层生长 (level-wise)的决策树生长策略，而使用了**带有深度限制的按叶子生长 (leaf-wise)**算法。

![img](https://www.biaodianfu.com/wp-content/uploads/2020/09/leaf-wise.png)

XGBoost采用的是按层生长level(depth)-wise生长策略，能够**同时分裂同一层的叶子，从而进行多线程优化，不容易过拟合**；但不加区分的对待同一层的叶子，带来了**很多没必要的开销**。因为实际上**很多叶子的分裂增益较低，没必要进行搜索和分裂**。

LightGBM采用leaf-wise生长策略，**每次从当前所有叶子中找到分裂增益最大**（一般也是数据量最大）的一个叶子，然后分裂，如此循环。因此同Level-wise相比，**在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度**。Leaf-wise的缺点是可能会**长出比较深的决策树，产生过拟合**。因此LightGBM在Leaf-wise之上增加了一个**最大深度的限制**，在保证高效率的同时防止过拟合。

### 直方图差加速

LightGBM另一个优化是Histogram（直方图）做差加速。一个容易观察到的现象：一个叶子的直方图可以由它的父亲节点的直方图与它兄弟的直方图做差得到。**通常构造直方图，需要遍历该叶子上的所有数据，但直方图做差仅需遍历直方图的k个桶**。利用这个方法，LightGBM可以在构造一个叶子的直方图后，可以用非常微小的代价得到它兄弟叶子的直方图，在速度上可以提升一倍。

![img](https://www.biaodianfu.com/wp-content/uploads/2020/09/lightgbm-histogram.png)

### 直接支持类别特征

实际上大多数机器学习工具都无法直接支持类别特征，一般需要把类别特征，转化one-hot特征，降低了空间和时间的效率。而类别特征的使用是在实践中很常用的。基于这个考虑，LightGBM优化了对类别特征的支持，可以直接输入类别特征，不需要额外的0/1展开。并在决策树算法上增加了类别特征的决策规则。

**one-hot编码是处理类别特征的一个通用方法**，然而在树模型中，这可能并不一定是一个好的方法，尤其当类别特征中类别个数很多的情况下。主要的问题是：

- 可能无法在这个类别特征上进行切分（即浪费了这个特征）。**使用one-hot编码的话，意味着在每一个决策节点上只能使用one vs rest**（例如是不是狗，是不是猫等）的切分方式。当类别值很多时，**每个类别上的数据可能会比较少，这时候切分会产生不平衡**，这意味着切分增益也会很小（比较直观的理解是，不平衡的切分和不切分没有区别）。
- 会影响决策树的学习。因为就算可以在这个类别特征进行切分，也会把数据切分到很多零碎的小空间上，如图1左边所示。而决策树学习时利用的是统计信息，在这些数据量小的空间上，统计信息不准确，学习会变差。但如果使用下图右边的分裂方式，数据会被切分到两个比较大的空间，进一步的学习也会更好。

 ![img](https://www.biaodianfu.com/wp-content/uploads/2020/09/node.jpg) 

**LightGBM处理分类特征大致流程：**

为了解决one-hot编码处理类别特征的不足。LightGBM采用了**Many vs many**的切分方式，实现了类别特征的最优切分。**用LightGBM可以直接输入类别特征，并产生上图右边的效果**。在1个k维的类别特征中寻找最优切分，朴素的枚举算法的复杂度是O(k2)，而LightGBM采用了如[On Grouping For Maximum Homogeneity](http://www.csiss.org/SPACE/workshops/2004/SAC/files/fisher.pdf)的方法实现了O(klogk)的算法。

算法流程下图所示：在枚举分割点之前，**先把直方图按每个类别的均值进行排序**；然后按照均值的结果依次枚举最优分割点。从下图可以看到，**Sum(y)/Count(y)为类别的均值**。当然，这个方法很容易过拟合，所以在LightGBM中加入了很多对这个方法的约束和正则化。

 ![img](https://www.biaodianfu.com/wp-content/uploads/2020/09/category.jpg) 

 下图是一个简单的对比实验，可以看到该最优方法在AUC上提升了1.5个点，并且时间只多了20%。 

 ![img](https://www.biaodianfu.com/wp-content/uploads/2020/09/auc-1.png) 

### 直接支持高效并行

LightGBM原生支持并行学习，目前支持**特征并行**和**数据并行**的两种。

**特征并行的主要思想是不同机器在不同的特征集合上分别寻找最优的分割点，然后在机器间同步最优的分割点。**

**数据并行则是让不同机器先在本地构造直方图，然后进行全局的合并，最后在合并的直方图上面寻找最优分割点。**

LightGBM针对这两种并行方法都做了优化，在特征并行算法中，通过在本地保存全部数据避免对数据切分结果的通信；

在数据并行中使用分散规约(Reduce scatter)把直方图合并的任务分摊到不同的机器，降低通信和计算，并利用直方图做差，进一步减少了一半的通信量。 

 ![img](https://www.biaodianfu.com/wp-content/uploads/2020/09/feature-parallelization.png)

 ![img](https://www.biaodianfu.com/wp-content/uploads/2020/09/data-parallelization.png)  

**基于投票的数据并行**则进一步优化数据并行中的通信代价，使通信代价变成常数级别。在数据量很大的时候，使用投票并行可以得到非常好的加速效果。 

 ![img](https://www.biaodianfu.com/wp-content/uploads/2020/09/voting-based-parallel.jpg) 

### 网络通信优化

XGBoost由于采用pre-sorted算法，通信代价非常大，所以在并行的时候也是采用histogram算法；

LightGBM采用的histogram算法通信代价小，通过使用集合通信算法，能够实现并行计算的线性加速。

## LightGBM原理

论文地址：[LightGBM A Highly Efficient Gradient Boosting](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf)

GBDT采用负梯度作为划分的指标（信息增益），XGBoost则利用到二阶导数。他们共同的不足是，计算信息增益需要扫描所有样本，从而找到最优划分点。在面对**大量数据或者特征维度很高**时，它们的效率和扩展性很难使人满意。解决这个问题的直接方法就是减少特征量和数据量而且不影响精确度。

微软开源的LightGBM（基于GBDT的）则很好的解决这些问题，它主要包含两个算法：

**单边梯度采样，Gradient-based One-Side Sampling（GOSS）**

GOSS（从减少样本角度）：**排除大部分小梯度的样本，仅用剩下的样本计算信息增益**。GBDT虽然没有数据权重，但每个数据实例有不同的梯度，根据计算信息增益的定义，**梯度大的实例对信息增益有更大的影响**，因此在下采样时，我们应该尽量保留梯度大的样本（预先设定阈值，或者最高百分位间），随机去掉梯度小的样本。我们证明**此措施在相同的采样率下比随机采样获得更准确的结果**，尤其是在信息增益范围较大时。

**互斥特征绑定，Exclusive Feature Bundling（EFB）**

- EFB（从减少特征角度）：捆绑互斥特征，也就是他们很少同时取非零值（也就是用一个合成特征代替）。通常真实应用中，虽然特征量比较多，但是由于特征空间十分稀疏，是否可以设计一种无损的方法来减少有效特征呢？特别在稀疏特征空间上，许多特征几乎是互斥的（例如许多特征不会同时为非零值，像one-hot），我们可以捆绑互斥的特征。最后，我们将捆绑问题归约到图着色问题，通过贪心算法求得近似解。

结合使用 GOSS 和 EFB 的 GBDT 算法就是 LightGBM。
