# Lasso回归、Ridge回归、ElasticNet回归

在处理较为复杂的数据的回归问题时，普通的[线性回归算法](https://www.biaodianfu.com/linear-regression.html)通常会出现预测精度不够，**如果模型中的特征之间有相关关系，就会增加模型的复杂程度**。当数据集中的特征之间有较强的线性相关性时，即特征之间出现严重的多重共线性时，**用普通最小二乘法估计模型参数，往往参数估计的方差太大**，此时，求解出来的模型就很不稳定。在具体取值上与真值有较大的偏差，有时会出现与实际意义不符的正负号。

在线性回归中如果参数θ过大、特征过多就会很容易造成过拟合，如下如所示：

![img](https://www.biaodianfu.com/wp-content/uploads/2020/09/overfitting.png)

## 正则化

Ridge回归与Lasso回归的出现是为了解决**线性回归出现的过拟合**以及在**通过正规方程方法求解θ的过程中出现的**
$$
(X^TX)
$$
**不可逆**这两类问题，这两种回归均通过在损失函数中引入正则化项来达到目的。

在日常机器学习任务中，如果数据集的特征比样本点还多
$$
 (X^TX)^{-1}
$$
的时候会出错。**Ridge回归最先用来处理特征数多于样本数的情况，现在也用于在估计中加入偏差**，从而得到更好的估计。这里通过引入λ限制了所有
$$
\theta^2
$$
之和，通过引入该惩罚项，能够减少不重要的参数，这个技术在统计学上也叫作缩减（shrinkage）。

和Ridge回归类似，另一个Lasso回归也加入了正则项对回归系数做了限定。

为了防止过拟合(θ过大)，在目标函数 J(θ) 后添加复杂度惩罚因子，即正则项来防止过拟合。

正则项可以使用L1-norm(Lasso)、L2-norm(Ridge)，或结合L1-norm、L2-norm(Elastic Net)。

**Lasso：使用L1-norm正则**
$$
J(\theta)=\frac{1}{2}\sum_{i}^{m}(y^{(i)}-\theta ^Tx^{(i)})^2+\lambda \sum_{j}^{n}|\theta_j|
$$
**Ridge：使用L2-norm正则**
$$
J(\theta)=\frac{1}{2}\sum_{i}^{m}(y^{(i)}-\theta ^Tx^{(i)})^2+\lambda \sum_{j}^{n}\theta_j^2
$$
**ElasticNet：结合L1-norm、L2-norm进行正则**
$$
J(\theta)=\frac{1}{2}\sum_{i}^{m}(y^{(i)}-\theta ^Tx^{(i)})^2+\lambda (\rho\sum_{j}^{n}|\theta_j|+(1-\rho)\sum_{j}^{n}\theta_j^2)
$$
简单的理解正则化：

1. 正则化的目的：防止过拟合
2. 正则化的本质：约束（限制）要优化的参数

关于第1点，过拟合指的是给定一堆数据，这堆数据带有噪声，利用模型去拟合这堆数据，可能会把噪声数据也给拟合了，这点很致命，一方面会造成模型比较复杂，另一方面，模型的泛化性能太差了，遇到了新的数据让你测试，你所得到的过拟合的模型，正确率是很差的。

关于第2点，**本来解空间是全部区域，但通过正则化添加了一些约束，使得解空间变小了，甚至在个别正则化方式下，解变得稀疏了**。这一点不得不提到一个图，相信我们都经常看到这个图，但貌似还没有一个特别清晰的解释，这里我尝试解释一下，图如下：

![img](https://www.biaodianfu.com/wp-content/uploads/2020/09/ridge-lasso.png)

上图中左边为Lasso回归，右边为Ridge回归。红色的椭圆和蓝色的区域的切点就是目标函数的最优解，我们可以看到，如果是圆，则很容易切到圆周的任意一点，但是很难切到坐标轴上，因此没有稀疏；但是如果是菱形或者多边形，则很容易切到坐标轴上，因此很容易产生稀疏的结果。这也说明了为什么**L1范式会是稀疏的**。这样就解释了为什么Lasso可以进行特征选择。**Ridge回归虽然不能进行特征筛选，但是对θ的模做约束，使得它的数值会比较小**，很大程度上减轻了overfitting的问题。

这里的β1，β2都是模型的参数，要优化的目标参数，蓝色部分区域，其实就是解空间，正如上面所说，这个时候，解空间“缩小了”，你只能在这个缩小了的空间中，寻找使得目标函数最小的β1，β2。再看看那红色的圆圈，再次提醒大家，这个坐标轴和特征（数据）没关系，它完全是参数的坐标系，每一个圆圈上，可以取无数个β1，β2，这些β1，β2有个共同的特点，用它们计算的目标函数值是相等的。那个红色的圆心，就是实际最优参数，但是由于我们对解空间做了限制，所以最优解只能在“缩小的”解空间中产生。

以两个变量为例，解释岭回归的几何意义:

1、没有约束项时。模型参数β1，β2已经经过标准化。残差平方和RSS可以表示为β1，β2的一个二次函数，数学上可以用一个抛物面表示。

![img](https://www.biaodianfu.com/wp-content/uploads/2020/09/ridge-1.jpg)

2、岭回归时。约束项为
$$
\beta _1^2 + \beta _2^2 \leq t
$$
对应着投影为β1，β2平面上的一个圆，即下图中的圆柱。

![img](https://www.biaodianfu.com/wp-content/uploads/2020/09/ridge-2.jpg)

可见岭回归解与原先的最小二乘解是有一定距离的。
